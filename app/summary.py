import os
import torch
from transformers import MT5Tokenizer, MT5ForConditionalGeneration
from peft import PeftModel

# ì„¤ì •
ADAPTER_PATH = "model/text_summary_model"
BASE_MODEL   = "google/mt5-base"
DEVICE       = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# í† í¬ë‚˜ì´ì € ë° mT5-LoRA ëª¨ë¸ ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer  = MT5Tokenizer.from_pretrained(ADAPTER_PATH)
base_model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL)
model      = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.to(DEVICE)
model.eval()
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")


def summarize_text(text: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1) ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (ëª…ì‹œì ìœ¼ë¡œ text í‚¤ì›Œë“œ ì‚¬ìš©)
    inputs = tokenizer(
        text=text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=512
    )
    # 2) í…ì„œë¥¼ ëª¨ë¸ì´ ìœ„ì¹˜í•œ ì¥ì¹˜ë¡œ ì´ë™
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    # 3) ìš”ì•½ ìƒì„±
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=256,
            min_length=30,
            decoder_start_token_id=tokenizer.pad_token_id,
            num_beams=5,
            no_repeat_ngram_size=6,
            early_stopping=True
        )

    # 4) ë””ì½”ë”© ë° íŠ¹ìˆ˜ í† í° ì œê±°
    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    summary = summary.replace("<extra_id_0>", "").strip()
    return summary
